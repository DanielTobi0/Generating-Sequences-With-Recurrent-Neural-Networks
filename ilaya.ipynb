{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python -m spacy download fr_core_news_sm\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:04.523209Z","iopub.execute_input":"2025-01-13T09:38:04.523511Z","iopub.status.idle":"2025-01-13T09:38:25.692408Z","shell.execute_reply.started":"2025-01-13T09:38:04.523488Z","shell.execute_reply":"2025-01-13T09:38:25.691361Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting fr-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.2)\nInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.7.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting en-core-web-sm==3.7.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport spacy\n# import gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:25.693709Z","iopub.execute_input":"2025-01-13T09:38:25.694002Z","iopub.status.idle":"2025-01-13T09:38:29.960754Z","shell.execute_reply.started":"2025-01-13T09:38:25.693973Z","shell.execute_reply":"2025-01-13T09:38:29.960095Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"''' \nThis cell performs the following actions.\n1. load data via huggingface\n2. convert to dataframe to split and convert to huggingface dataset format.\n'''\n\ndata_files = {'validation': 'fr-en/validation-00000-of-00001.parquet'}\ndataset = load_dataset(path='wmt/wmt14', trust_remote_code=True, data_files=data_files)\ndata = pd.DataFrame(dataset['validation'])\n\ntrain, temp = train_test_split(data, test_size=0.4, random_state=0)\ntest, validation = train_test_split(temp, test_size=0.5, random_state=0)\n\ndef process_translations(df):\n    en_texts = [item['en'] for item in df['translation']]\n    fr_texts = [item['fr'] for item in df['translation']]\n    \n    return pd.DataFrame({\n        'en': en_texts,\n        'fr': fr_texts\n    })\n\ntrain_processed = process_translations(train)\ntest_processed = process_translations(test)\nvalidation_processed = process_translations(validation)\n\ntrain_dataset = Dataset.from_pandas(train_processed.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_processed.reset_index(drop=True))\nvalidation_dataset = Dataset.from_pandas(validation_processed.reset_index(drop=True))\n\nds = DatasetDict({\n    'train': train_dataset,\n    'test': test_dataset,\n    'validation': validation_dataset\n})\n\nds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:29.962579Z","iopub.execute_input":"2025-01-13T09:38:29.963115Z","iopub.status.idle":"2025-01-13T09:38:33.425131Z","shell.execute_reply.started":"2025-01-13T09:38:29.963079Z","shell.execute_reply":"2025-01-13T09:38:33.424454Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"972011daf0ac4f6aa785df1c20b07a7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/475k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb810bc865844f56872b4675ad6d2718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb45f881502347a29689587d8aef846a"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['en', 'fr'],\n        num_rows: 1800\n    })\n    test: Dataset({\n        features: ['en', 'fr'],\n        num_rows: 600\n    })\n    validation: Dataset({\n        features: ['en', 'fr'],\n        num_rows: 600\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# del dataset, data\n# del train, temp, test, validation\n# del process_translations, train_processed, test_processed, validation_dataset\n# gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:33.426225Z","iopub.execute_input":"2025-01-13T09:38:33.426481Z","iopub.status.idle":"2025-01-13T09:38:33.429660Z","shell.execute_reply.started":"2025-01-13T09:38:33.426459Z","shell.execute_reply":"2025-01-13T09:38:33.428798Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''\nThis cell performs the following actions.\n1. load the english and french spacy tokenizer\n2. set the special characters.\n3. tokenizer the whole dataset and split into train, test and validation\n'''\n\nen_nlp = spacy.load('en_core_web_sm')\nfr_nlp = spacy.load('fr_core_news_sm')\n\ndef tokenize_example(example, en_nlp, fr_nlp, max_length, sos_token, eos_token):\n    en_tokens = [token.text.lower() for token in en_nlp.tokenizer(example['en'])][:max_length]\n    fr_tokens = [token.text.lower() for token in fr_nlp.tokenizer(example['fr'])][:max_length]\n\n    en_tokens = [sos_token] + en_tokens + [eos_token]\n    fr_tokens = [sos_token] + fr_tokens + [eos_token]\n\n    return {'en_tokens': en_tokens, 'fr_tokens': fr_tokens} \n\n    \nmax_length = 1000\nsos_token = '<sos>'\neos_token = '<eos>'\npad_token = '<pad>'\n\nfn_kwargs = {\n    'en_nlp': en_nlp,\n    'fr_nlp': fr_nlp,\n    'max_length': max_length,\n    'sos_token': sos_token,\n    'eos_token': eos_token,\n}\n\ntrain_data, test_data, validation_data = (\n    ds['train'],\n    ds['test'],\n    ds['validation'],\n)\n\ntrain_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\ntest_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\nvalidation_data = validation_data.map(tokenize_example, fn_kwargs=fn_kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:33.430606Z","iopub.execute_input":"2025-01-13T09:38:33.430876Z","iopub.status.idle":"2025-01-13T09:38:43.067549Z","shell.execute_reply.started":"2025-01-13T09:38:33.430855Z","shell.execute_reply":"2025-01-13T09:38:43.066647Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a710b8cb864394ad6e1b00e588237f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c92f3c5efe48019af0c7aee27c78ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d9ebce19c74ccaae70394066004325"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"''' \nCreate English and French language vocabulary.\nMapping of word to number(integer)\n'''\n\nfrom collections import Counter\n\ndef lang_str_int(lang, nlp):\n    lang_vocab = []\n    special_vocab = ['<unk>', '<pad>', '<sos>', '<eos>'] \n\n    flattened_list = [token.text.lower() for sentence in lang for token in nlp.tokenizer(sentence)]\n    lang_count = Counter(flattened_list)\n    lang_words = [string for string, freq in lang_count.items() if freq >= 2]\n\n    lang_vocab = special_vocab + lang_words\n    # lang_vocab.extend(special_vocab)\n    # lang_vocab.extend(lang_words)\n\n    lang_str2int = {ch: i for i, ch in enumerate(lang_vocab)}\n    lang_int2str = {i: ch for i, ch in enumerate(lang_vocab)}\n\n    return lang_str2int, lang_int2str\n\nen = process_translations(data)['en'].tolist()\nfr = process_translations(data)['fr'].tolist()\n\nfr_str2int, fr_int2str = lang_str_int(fr, fr_nlp)\nen_str2int, en_int2str = lang_str_int(en, en_nlp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:43.068446Z","iopub.execute_input":"2025-01-13T09:38:43.068773Z","iopub.status.idle":"2025-01-13T09:38:43.341806Z","shell.execute_reply.started":"2025-01-13T09:38:43.068739Z","shell.execute_reply":"2025-01-13T09:38:43.341128Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"''' \ncreate a new feature of tokens(words) to numbers(integers).\n'''\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\n\ndef token_to_int(example, str2int):\n    return [str2int.get(token, str2int['<unk>']) for token in example]\n\ndef tokens_to_ids(example):\n    example['en_ids'] = token_to_int(example['en_tokens'], en_str2int)\n    example['fr_ids'] = token_to_int(example['fr_tokens'], fr_str2int)\n    return example\n\ntrain_data = train_data.map(tokens_to_ids)\ntest_data = test_data.map(tokens_to_ids)\nvalidation_data = validation_data.map(tokens_to_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:43.342563Z","iopub.execute_input":"2025-01-13T09:38:43.342834Z","iopub.status.idle":"2025-01-13T09:38:43.939110Z","shell.execute_reply.started":"2025-01-13T09:38:43.342804Z","shell.execute_reply":"2025-01-13T09:38:43.938206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c58378e7de431083c7bf8906532084"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec62f024167f4ec9a15150ff18c67006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79d79d6cce8f4920aea0ea155cb20351"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"'''\nReverse the source language, eg source_lang: 'hello' -> 'olleh'\nThis is said to give a significant boost in the model accuracy.\nexample: \n    source_lang: a, b, c\n    target_lang: α, β, γ\n\n    reversing: c, b, a -> α, β, γ\n    Why? because it makes it faster to establish a communication.\n'''\n\ndef reverse_source_lang(example):\n    '''reverse list'''\n    example['en_ids'] = example['en_ids'][::-1]\n    return example\n\ntrain_data = train_data.map(reverse_source_lang)\ntest_data = test_data.map(reverse_source_lang)\nvalidation_data = validation_data.map(reverse_source_lang)\n\ntrain_data.set_format(\n    type='torch',\n    columns=['en_ids', 'fr_ids'],\n    output_all_columns=False\n)\ntest_data.set_format(\n    type='torch',\n    columns=['en_ids', 'fr_ids'],\n    output_all_columns=False\n)\nvalidation_data.set_format(\n    type='torch',\n    columns=['en_ids', 'fr_ids'],\n    output_all_columns=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:43.941236Z","iopub.execute_input":"2025-01-13T09:38:43.941461Z","iopub.status.idle":"2025-01-13T09:38:44.233965Z","shell.execute_reply.started":"2025-01-13T09:38:43.941442Z","shell.execute_reply":"2025-01-13T09:38:44.233147Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49dbc628533c4ae195a7637cf760d3a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7cc2a678b5940c99a285f7fdd0f71eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ce7ac2093f54cd4b6bdbb58c8f903c4"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# data batching\ndef get_collate_fn(pad_index):\n    def collate_fn(batch):\n        batch_en_ids = [example['en_ids'] for example in batch]\n        batch_fr_ids = [example['fr_ids'] for example in batch]\n        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n        batch_fr_ids = nn.utils.rnn.pad_sequence(batch_fr_ids, padding_value=pad_index)\n        batch = {\n            'en_ids': batch_en_ids,\n            'fr_ids': batch_fr_ids\n        }\n        return batch\n    return collate_fn\n\n\ndef get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n    collate_fn = get_collate_fn(pad_index)\n    data_loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        collate_fn=collate_fn,\n        shuffle=shuffle\n    )\n    return data_loader\n\n\n# data loader\nbatch_size = 128\nPAD_INDEX = en_str2int[pad_token]\n\ntrain_data_loader = get_data_loader(train_data, batch_size, PAD_INDEX, shuffle=True)\ntest_data_loader = get_data_loader(test_data, batch_size, PAD_INDEX, shuffle=False)\nvalidation_data_loader = get_data_loader(validation_data, batch_size, PAD_INDEX, shuffle=False)\n\nresult = next(iter(train_data_loader))\nresult['en_ids'].shape, result['fr_ids'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:44.235166Z","iopub.execute_input":"2025-01-13T09:38:44.235476Z","iopub.status.idle":"2025-01-13T09:38:44.311984Z","shell.execute_reply.started":"2025-01-13T09:38:44.235445Z","shell.execute_reply":"2025-01-13T09:38:44.311132Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(torch.Size([72, 128]), torch.Size([82, 128]))"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Model architecture","metadata":{"execution":{"iopub.status.busy":"2025-01-13T09:09:13.666050Z","iopub.execute_input":"2025-01-13T09:09:13.666423Z","iopub.status.idle":"2025-01-13T09:09:13.670899Z","shell.execute_reply.started":"2025-01-13T09:09:13.666396Z","shell.execute_reply":"2025-01-13T09:09:13.669651Z"}}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n        super().__init__()\n     \n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, dropout=dropout)\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, cell):\n        input_ = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input_))\n        output, (hidden, cell) = self.rnn(embedded, (hidden,cell))\n        prediction = self.fc_out(output.squeeze(0))\n        return prediction, hidden, cell\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, trg, trg_len):\n        ''' \n        src: [src_len, batch_size]\n        trg: [trg_len, batch_size]\n        trg_len: length o\n        '''\n        batch_size = src.shape[1]\n        trg_vocab_size = self.decoder.output_dim\n\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        \n        hidden, cell = self.encoder(src)\n\n        input_ = trg[0, :]\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input_, hidden, cell)\n            outputs[t] = output            \n            top1 = output.argmax(1)\n            input_ = top1\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:44.312885Z","iopub.execute_input":"2025-01-13T09:38:44.313156Z","iopub.status.idle":"2025-01-13T09:38:44.321782Z","shell.execute_reply.started":"2025-01-13T09:38:44.313134Z","shell.execute_reply":"2025-01-13T09:38:44.320960Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"'''\nmodel hyperparameters\n'''\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ninput_dim = len(en_str2int)\noutput_dim = len(fr_str2int)\n\nencoder_embedding_dim = 256\ndecoder_embedding_dim = 256\n\nhidden_dim = 512\nn_layers = 2\n\nencoder_dropout = 0.5\ndecoder_dropout = 0.5\nCLIP = 1\n\nencoder = Encoder(\n    input_dim, \n    encoder_embedding_dim, \n    hidden_dim, \n    n_layers,\n    encoder_dropout\n).to(device)\n\ndecoder = Decoder(\n    output_dim, \n    decoder_embedding_dim, \n    hidden_dim, \n    n_layers,\n    decoder_dropout\n).to(device)\n\n\n''' \nThe paper suggest that initialzing the weight with a uniform distribution of -0.08 to 0.08\n'''\nfor param in encoder.parameters():\n    torch.nn.init.uniform_(param, a=-0.08, b=0.08)\n\nfor param in decoder.parameters():\n    torch.nn.init.uniform_(param, a=-0.08, b=0.08)\n    \nmodel = Seq2Seq(encoder, decoder, device).to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.7) \ncriterion = nn.CrossEntropyLoss(ignore_index=fr_str2int['<pad>'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:44.322674Z","iopub.execute_input":"2025-01-13T09:38:44.322935Z","iopub.status.idle":"2025-01-13T09:38:45.363683Z","shell.execute_reply.started":"2025-01-13T09:38:44.322904Z","shell.execute_reply":"2025-01-13T09:38:45.362786Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, clip):\n    model.train()\n\n    epoch_loss = 0\n\n    for batch in dataloader:\n        src = batch['en_ids'].to(device)\n        trg = batch['fr_ids'].to(device)\n\n        optimizer.zero_grad()\n\n        output = model(src, trg, trg.shape[0])\n\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].reshape(-1)\n\n        loss = criterion(output, trg)\n        loss.backward()\n\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    return epoch_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n\n    epoch_loss = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            src = batch['en_ids'].to(device)\n            trg = batch['fr_ids'].to(device)\n\n            output = model(src, trg, trg.shape[0])            \n\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].reshape(-1)\n\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:38:45.364548Z","iopub.execute_input":"2025-01-13T09:38:45.365096Z","iopub.status.idle":"2025-01-13T09:38:45.371493Z","shell.execute_reply.started":"2025-01-13T09:38:45.365045Z","shell.execute_reply":"2025-01-13T09:38:45.370652Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import time\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    mins = int(elapsed_time / 60)\n    secs = int(elapsed_time - (mins * 60))\n    return mins, secs\n\nN_EPOCHS = 7.5\ninitial_lr = 0.7\nhalf_epoch_counter = 0\nCLIP = 1\n\nfor epoch in range(int(N_EPOCHS)):\n    start_time = time.time()\n    \n    train_loss = train(model, train_data_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, validation_data_loader, criterion)\n    \n\n    if epoch >= 5:\n        half_epoch_counter += 1\n        \n        if half_epoch_counter % 1 == 0:\n            new_lr = optimizer.param_groups[0]['lr'] / 2\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = new_lr\n\n        # elif epoch % 0.5 == 0:\n        #     new_lr = optimizer.param_groups[0]['lr'] / 2\n        #     for param_group in optimizer.param_groups:\n        #         param_group['lr'] = new_lr\n\n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    \n    print(f'Epoch: {epoch+1:02} | Learning Rate: {optimizer.param_groups[0][\"lr\"]} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f}')\n# torch.save(model.state_dict(), 'seq2seq_model_no_teacher_forcing.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:43:14.546005Z","iopub.execute_input":"2025-01-13T09:43:14.546369Z","iopub.status.idle":"2025-01-13T09:44:01.247265Z","shell.execute_reply.started":"2025-01-13T09:43:14.546345Z","shell.execute_reply":"2025-01-13T09:44:01.246399Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 | Learning Rate: 0.7 | Time: 0m 6s\n\tTrain Loss: 5.998\n\t Val. Loss: 6.012\nEpoch: 02 | Learning Rate: 0.7 | Time: 0m 6s\n\tTrain Loss: 5.994\n\t Val. Loss: 6.044\nEpoch: 03 | Learning Rate: 0.7 | Time: 0m 6s\n\tTrain Loss: 6.025\n\t Val. Loss: 5.952\nEpoch: 04 | Learning Rate: 0.7 | Time: 0m 6s\n\tTrain Loss: 5.957\n\t Val. Loss: 5.965\nEpoch: 05 | Learning Rate: 0.7 | Time: 0m 6s\n\tTrain Loss: 5.915\n\t Val. Loss: 5.967\nEpoch: 06 | Learning Rate: 0.35 | Time: 0m 6s\n\tTrain Loss: 5.918\n\t Val. Loss: 5.966\nEpoch: 07 | Learning Rate: 0.175 | Time: 0m 6s\n\tTrain Loss: 5.893\n\t Val. Loss: 5.900\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"torch.save(model.state_dict(), 'seq2seq_model_no_teacher_forcing.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:44:01.248232Z","iopub.execute_input":"2025-01-13T09:44:01.248545Z","iopub.status.idle":"2025-01-13T09:44:01.330313Z","shell.execute_reply.started":"2025-01-13T09:44:01.248498Z","shell.execute_reply":"2025-01-13T09:44:01.329375Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def translate_sentence(sentence, encoder, decoder, src_vocab, trg_vocab, device, max_len=50):\n    \"\"\"\n    sentence: list of token indices\n    src_vocab: mapping from indices to tokens for source language\n    trg_vocab: mapping from indices to tokens for target language\n    \"\"\"\n    encoder.eval()\n    decoder.eval()\n    \n    # Convert to tensor and add batch dimension\n    src_tensor = torch.LongTensor(sentence).unsqueeze(1).to(device)  # [src_len, 1]\n    \n    with torch.no_grad():\n        hidden, cell = encoder(src_tensor)\n    \n    # Start with <sos> token\n    trg_indexes = [fr_str2int['<sos>']]\n    \n    for _ in range(max_len):\n        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n        \n        with torch.no_grad():\n            output, hidden, cell = decoder(trg_tensor, hidden, cell)\n        \n        pred_token = output.argmax(1).item()\n        trg_indexes.append(pred_token)\n        \n        if pred_token == fr_str2int['<eos>']:\n            break\n    \n    # Convert indices to tokens\n    trg_tokens = [fr_int2str[token] for token in trg_indexes]\n    \n    return trg_tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:44:01.331886Z","iopub.execute_input":"2025-01-13T09:44:01.332248Z","iopub.status.idle":"2025-01-13T09:44:01.337775Z","shell.execute_reply.started":"2025-01-13T09:44:01.332213Z","shell.execute_reply":"2025-01-13T09:44:01.336937Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"sample_sentence = [\n    en_str2int['<sos>'], \n    en_str2int['i'], \n    en_str2int['am'], \n    en_str2int['a'], \n    en_str2int['student'], \n    en_str2int['<eos>']\n]\n\n# Translate\ntranslation = translate_sentence(sample_sentence, encoder, decoder, en_str2int, fr_str2int, device)\n\nprint('Translated French Sentence:', ' '.join(translation))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:44:24.445900Z","iopub.execute_input":"2025-01-13T09:44:24.446220Z","iopub.status.idle":"2025-01-13T09:44:24.468709Z","shell.execute_reply.started":"2025-01-13T09:44:24.446197Z","shell.execute_reply":"2025-01-13T09:44:24.468010Z"}},"outputs":[{"name":"stdout","text":"Translated French Sentence: <sos> les <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <eos>\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n\ndef calculate_bleu(data, model, device, max_len=50):\n    trgs = []\n    pred_trgs = []\n    \n    for datum in data:\n        src = datum['en_ids']\n        trg = datum['fr_ids']\n        \n        src_sentence = src.tolist()[0]\n        trg_sentence = trg.tolist()[0]\n        \n        pred_tokens = translate_sentence(src_sentence, model.encoder, model.decoder, en_str2int, fr_str2int, device, max_len)\n        pred_tokens = pred_tokens[1:-1]  # Remove <sos> and <eos>\n        \n        trg_tokens = [int2fr[token] for token in trg_sentence if token not in [fr_str2int['<sos>'], fr_str2int['<eos>'], fr_str2int['<pad>']]]\n        \n        trgs.append([trg_tokens])\n        pred_trgs.append(pred_tokens)\n    \n    return corpus_bleu(trgs, pred_trgs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:44:30.227030Z","iopub.execute_input":"2025-01-13T09:44:30.227376Z","iopub.status.idle":"2025-01-13T09:44:30.645552Z","shell.execute_reply.started":"2025-01-13T09:44:30.227352Z","shell.execute_reply":"2025-01-13T09:44:30.644651Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"calculate_bleu(validation_data_loader, model, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-13T09:44:31.813347Z","iopub.execute_input":"2025-01-13T09:44:31.813657Z","iopub.status.idle":"2025-01-13T09:44:31.923139Z","shell.execute_reply.started":"2025-01-13T09:44:31.813631Z","shell.execute_reply":"2025-01-13T09:44:31.922244Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Saving\ntorch.save({\n    'encoder_state_dict': encoder.state_dict(),\n    'decoder_state_dict': decoder.state_dict(),\n}, 'seq2seq_model.pth')\n\n# Loading\ncheckpoint = torch.load('seq2seq_model.pth')\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\nmodel = Seq2Seq(encoder, decoder, device).to(device)\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}