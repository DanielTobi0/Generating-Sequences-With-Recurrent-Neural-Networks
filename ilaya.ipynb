{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\HomePC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\HomePC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\HomePC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_12424\\986851973.py\", line 7, in <module>\n",
      "    import spacy\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\thinc\\types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\thinc\\compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\HomePC\\Desktop\\papers_implementation\\Generating Sequences With Recurrent Neural Networks\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download fr_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import spacy\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 1800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {'validation': 'fr-en/validation-00000-of-00001.parquet'}\n",
    "dataset = load_dataset(path='wmt/wmt14', trust_remote_code=True, data_files=data_files)\n",
    "data = pd.DataFrame(dataset['validation'])\n",
    "\n",
    "train, temp = train_test_split(data, test_size=0.4, random_state=0)\n",
    "test, validation = train_test_split(temp, test_size=0.5, random_state=0)\n",
    "\n",
    "def process_translations(df):\n",
    "    en_texts = [item['en'] for item in df['translation']]\n",
    "    fr_texts = [item['fr'] for item in df['translation']]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'en': en_texts,\n",
    "        'fr': fr_texts\n",
    "    })\n",
    "\n",
    "train_processed = process_translations(train)\n",
    "test_processed = process_translations(test)\n",
    "validation_processed = process_translations(validation)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_processed.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_processed.reset_index(drop=True))\n",
    "validation_dataset = Dataset.from_pandas(validation_processed.reset_index(drop=True))\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "    'validation': validation_dataset\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del dataset, data\n",
    "# del train, temp, test, validation\n",
    "# del process_translations, train_processed, test_processed, validation_dataset\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50ada20eb0845948ec13570156b5a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9595697bfb4d9a9aad36f557c7d54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2052fbd751724fb1a0a3aceec5e7a319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "fr_nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def tokenize_example(example, en_nlp, fr_nlp, max_length, sos_token, eos_token):\n",
    "    en_tokens = [token.text.lower() for token in en_nlp.tokenizer(example['en'])][:max_length]\n",
    "    fr_tokens = [token.text.lower() for token in fr_nlp.tokenizer(example['fr'])][:max_length]\n",
    "\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    fr_tokens = [sos_token] + fr_tokens + [eos_token]\n",
    "\n",
    "    return {'en_tokens': en_tokens, 'fr_tokens': fr_tokens} \n",
    "\n",
    "    \n",
    "max_length = 1000\n",
    "sos_token = '<sos>'\n",
    "eos_token = '<eos>'\n",
    "\n",
    "fn_kwargs = {\n",
    "    'en_nlp': en_nlp,\n",
    "    'fr_nlp': fr_nlp,\n",
    "    'max_length': max_length,\n",
    "    'sos_token': sos_token,\n",
    "    'eos_token': eos_token,\n",
    "}\n",
    "\n",
    "train_data, test_data, validation_data = (\n",
    "    ds['train'],\n",
    "    ds['test'],\n",
    "    ds['validation'],\n",
    ")\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "validation_data = validation_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def lang_str_int(lang, nlp):\n",
    "    lang_vocab = []\n",
    "    special_vocab = ['<unk>', '<pad>', '<sos>', '<eos>'] \n",
    "\n",
    "    flattened_list = [token.text.lower() for sentence in lang for token in nlp.tokenizer(sentence)]\n",
    "    lang_count = Counter(flattened_list)\n",
    "    lang_words = [string for string, freq in lang_count.items() if freq >= 2]\n",
    "\n",
    "    lang_vocab = special_vocab + lang_words\n",
    "    # lang_vocab.extend(special_vocab)\n",
    "    # lang_vocab.extend(lang_words)\n",
    "\n",
    "    lang_str2int = {ch: i for i, ch in enumerate(lang_vocab)}\n",
    "    lang_int2str = {i: ch for i, ch in enumerate(lang_vocab)}\n",
    "\n",
    "    return lang_str2int, lang_int2str\n",
    "\n",
    "en = process_translations(data)['en'].tolist()\n",
    "fr = process_translations(data)['fr'].tolist()\n",
    "\n",
    "fr_str2int, fr_int2str = lang_str_int(fr, fr_nlp)\n",
    "en_str2int, en_int2str = lang_str_int(en, en_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f9284850c04048bf7f68a07efa53d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7571af1faac64a799485aaffa8af7cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6df89597cc42c9bb3091296592524f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def token_to_int(example, str2int):\n",
    "    hh = [str2int.get(token, str2int['<unk>']) for token in example]\n",
    "    return hh\n",
    "\n",
    "def tokens_to_ids(example):\n",
    "    example['en_ids'] = token_to_int(example['en_tokens'], en_str2int)\n",
    "    example['fr_ids'] = token_to_int(example['fr_tokens'], fr_str2int)\n",
    "    return example\n",
    "\n",
    "train_data = train_data.map(tokens_to_ids)\n",
    "test_data = test_data.map(tokens_to_ids)\n",
    "validation_data = validation_data.map(tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cast_torch(example):\n",
    "#     example['en_ids'] = torch.tensor(example['en_ids'], dtype=torch.long)\n",
    "#     return example\n",
    "\n",
    "# train_data.map(cast_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 57, 9, 1022, 96, 53, 1138, 210, 1139, 26, 850, 461, 850, 457, 32, 93, 4, 0, 33, 416, 7, 1140, 60, 1009, 9, 67, 11, 0, 19, 0, 11, 1101, 26, 7, 289, 4, 1141, 7, 9, 0, 120, 0, 9, 0, 24, 457, 3]\n"
     ]
    }
   ],
   "source": [
    "check = train_data[999]['en_ids']\n",
    "print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
